{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf7569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SMSClassifierModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__();\n",
    "        self.model_input = nn.Embedding(vocab_size, 100);\n",
    "        # self.rnn = nn.RNN(100, 128);\n",
    "        self.rnn = nn.LSTM(100, 128);\n",
    "        self.model_output = nn.Linear(128, 1);\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, sms_text):\n",
    "        embedded = self.model_input(sms_text);\n",
    "        # _,hidden = self.rnn(embedded);\n",
    "        _,(hidden,_) = self.rnn(embedded);\n",
    "        output = self.model_output(hidden);\n",
    "        return self.activation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f80a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "def read_vocab(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    vocab = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return vocab\n",
    "\n",
    "def load_model_and_vocab(vocab_path, weights_path):    \n",
    "    vocab = read_vocab(vocab_path)\n",
    "    model = SMSClassifierModel(vocab_size=len(vocab))\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    model.eval()\n",
    "    return model, vocab\n",
    "\n",
    "model, vocab = load_model_and_vocab(\"sms_vocab.pkl\", \"sms_classifier_weights.pt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62fdf0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\");\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4685b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 1115])\n",
      "0 spam\n",
      "1 spam\n",
      "2 spam\n",
      "3 spam\n",
      "4 spam\n",
      "5 spam\n",
      "6 spam\n",
      "7 spam\n",
      "8 spam\n",
      "9 spam\n",
      "10 spam\n",
      "11 spam\n",
      "12 spam\n",
      "13 spam\n",
      "14 spam\n",
      "15 spam\n",
      "16 spam\n",
      "17 spam\n",
      "18 spam\n",
      "19 spam\n",
      "20 spam\n",
      "21 spam\n",
      "22 spam\n",
      "23 spam\n",
      "24 spam\n",
      "25 spam\n",
      "26 spam\n",
      "27 spam\n",
      "28 spam\n",
      "29 spam\n",
      "30 spam\n",
      "31 spam\n",
      "32 spam\n",
      "33 spam\n",
      "34 spam\n",
      "35 spam\n",
      "36 spam\n",
      "37 spam\n",
      "38 spam\n",
      "39 spam\n",
      "40 spam\n",
      "41 spam\n",
      "42 spam\n",
      "43 spam\n",
      "44 spam\n",
      "45 spam\n",
      "46 spam\n",
      "47 spam\n",
      "48 spam\n",
      "49 spam\n",
      "50 spam\n",
      "51 spam\n",
      "52 spam\n",
      "53 spam\n",
      "54 spam\n",
      "55 spam\n",
      "56 spam\n",
      "57 spam\n",
      "58 spam\n",
      "59 spam\n",
      "60 spam\n",
      "61 spam\n",
      "62 spam\n",
      "63 spam\n",
      "64 spam\n",
      "65 spam\n",
      "66 spam\n",
      "67 spam\n",
      "68 spam\n",
      "69 spam\n",
      "70 spam\n",
      "71 spam\n",
      "72 spam\n",
      "73 spam\n",
      "74 spam\n",
      "75 spam\n",
      "76 spam\n",
      "77 spam\n",
      "78 spam\n",
      "79 spam\n",
      "80 spam\n",
      "81 spam\n",
      "82 spam\n",
      "83 spam\n",
      "84 spam\n",
      "85 spam\n",
      "86 spam\n",
      "87 spam\n",
      "88 spam\n",
      "89 spam\n",
      "90 spam\n",
      "91 spam\n",
      "92 spam\n",
      "93 spam\n",
      "94 spam\n",
      "95 spam\n",
      "96 spam\n",
      "97 spam\n",
      "98 spam\n",
      "99 spam\n",
      "100 spam\n",
      "101 spam\n",
      "102 spam\n",
      "103 spam\n",
      "104 spam\n",
      "105 spam\n",
      "106 spam\n",
      "107 spam\n",
      "108 spam\n",
      "109 spam\n",
      "110 spam\n",
      "111 spam\n",
      "112 spam\n",
      "113 spam\n",
      "114 spam\n",
      "115 spam\n",
      "116 spam\n",
      "117 spam\n",
      "118 spam\n",
      "119 spam\n",
      "120 spam\n",
      "121 spam\n",
      "122 spam\n",
      "123 spam\n",
      "124 spam\n",
      "125 spam\n",
      "126 spam\n",
      "127 spam\n",
      "128 spam\n",
      "129 spam\n",
      "130 spam\n",
      "131 spam\n",
      "132 spam\n",
      "133 spam\n",
      "134 spam\n",
      "135 spam\n",
      "136 spam\n",
      "137 spam\n",
      "138 spam\n",
      "139 spam\n",
      "140 spam\n",
      "141 spam\n",
      "142 spam\n",
      "143 spam\n",
      "144 spam\n",
      "145 spam\n",
      "146 spam\n",
      "147 spam\n",
      "148 spam\n",
      "149 spam\n",
      "86.54708520179372 %\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/sms-classification/spam_test.csv\", encoding=\"latin-1\");\n",
    "data.head()\n",
    "indexed = []\n",
    "for index, row in data.iterrows():    \n",
    "    tokenized = word_tokenize(row['text']);\n",
    "    index_sentence = torch.LongTensor([vocab.stoi[t] for t in tokenized]);\n",
    "    indexed.append(index_sentence);\n",
    "    \n",
    "indexed_padded = pad_sequence(indexed, batch_first=True, padding_value=1)\n",
    "input_tensor = torch.LongTensor(indexed_padded).transpose(0,1);\n",
    "print(input_tensor.shape); # [sentence_length, batch_size]\n",
    "pred = model(input_tensor).squeeze();\n",
    "correct = 0\n",
    "n = 0\n",
    "for i,p in enumerate(pred):\n",
    "    p = torch.round(p);\n",
    "    label = {0: \"ham\", 1:\"spam\"};\n",
    "    label_rev = {\"ham\":0, \"spam\":1};\n",
    "    correct += p.item()==label_rev[data.iloc[i]['label']]\n",
    "    if not p.item()==label_rev[data.iloc[i]['label']]:\n",
    "        print(n, data.iloc[i]['label'])\n",
    "        n+=1\n",
    "#     print(data.iloc[i]['text'], \" => \", label[p.item()], \" \", data.iloc[i]['label']);\n",
    "print(correct*100/len(data.index),\"%\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
