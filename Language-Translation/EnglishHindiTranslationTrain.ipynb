{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b9ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, name):\n",
    "        self.word2index = {'<START>': 0, '<END>': 1, '<UNK>': 2}\n",
    "        self.index2word = {0: '<START>', 1: '<END>', 2: '<UNK>'}\n",
    "        self.word_freq = {}\n",
    "        self.n_words = 3\n",
    "        \n",
    "    def add_sentence(self, sentence):        \n",
    "        for word in sentence.split(' '):            \n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word_freq[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word_freq[word] += 1\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = re.sub(r'([!.?,ред])', r' \\1', sentence)\n",
    "    sentence = re.sub(r'\\n', r'', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b2d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Data:  1659083\n",
      "Total Testing Data:  2507\n"
     ]
    }
   ],
   "source": [
    "path = '../datasets/english-hindi-translation/'\n",
    "train_set = 'train'\n",
    "test_set = 'test'\n",
    "\n",
    "source = open(path+'source_' + train_set + '.txt', encoding='utf8')\n",
    "source_lang = Lang('eng')\n",
    "\n",
    "target = open(path+'target_' + train_set + '.txt', encoding='utf8')\n",
    "target_lang = Lang('hin')\n",
    "\n",
    "pairs_train = []\n",
    "for src, tar in zip(source.readlines(), target.readlines()):\n",
    "    src = clean_sentence(src)\n",
    "    tar = clean_sentence(tar)\n",
    "    source_lang.add_sentence(src)\n",
    "    target_lang.add_sentence(tar)\n",
    "    pairs_train.append((src, tar))\n",
    "\n",
    "source = open(path+'source_' + test_set + '.txt', encoding='utf8')\n",
    "target = open(path+'target_' + test_set + '.txt', encoding='utf8')\n",
    "\n",
    "pairs_test = []\n",
    "for src, tar in zip(source.readlines(), target.readlines()):\n",
    "    pairs_test.append((src, tar))\n",
    "\n",
    "print('Total Training Data: ', len(pairs_train))\n",
    "print('Total Testing Data: ', len(pairs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41eaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__();\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, line, hidden):\n",
    "        embedded = self.embedding(line)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__();\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)        \n",
    "    \n",
    "    # Need to process word by word to implement teacher forcing\n",
    "    def forward(self, word, hidden):\n",
    "        embedded = self.embedding(word)\n",
    "        activated = self.relu(embedded)\n",
    "        output, hidden = self.rnn(activated, hidden)\n",
    "        output = self.softmax(self.linear(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor_pair(sentence_pair):\n",
    "    source_tensor = [source_lang.word2index[x] if x in source_lang.word2index else source_lang.word2index['<UNK>'] for x in sentence_pair[0].split(' ')]\n",
    "    source_tensor.append(source_lang.word2index['<END>'])\n",
    "    source_tensor = torch.tensor(source_tensor)\n",
    "    target_tensor = [target_lang.word2index[x] if x in target_lang.word2index else target_lang.word2index['<UNK>'] for x in sentence_pair[1].split(' ')]\n",
    "    target_tensor.append(target_lang.word2index['<END>'])\n",
    "    target_tensor = torch.tensor(target_tensor)\n",
    "    return source_tensor, target_tensor\n",
    "\n",
    "for pair in pairs_train[0:2]:\n",
    "    print(sentence2tensor_pair(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "encoder = EncoderRNN(vocab_dim = source_lang.n_words,\n",
    "                     embedding_dim = 100,\n",
    "                     hidden_dim = 50)\n",
    "decoder = DecoderRNN(vocab_dim = target_lang.n_words,\n",
    "                     embedding_dim = 100,\n",
    "                     hidden_dim = 50)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.005);\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.005);\n",
    "lossfn = nn.NLLLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_pair = sentence2tensor_pair(pairs_train[0])\n",
    "\n",
    "hidden = encoder.init_hidden()\n",
    "input_ = tensor_pair[0].unsqueeze(1)\n",
    "print(input_.shape)\n",
    "output, hidden = encoder(input_, hidden)\n",
    "print(hidden.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.tensor([target_lang.word2index['<START>']]).unsqueeze(1)\n",
    "print(input_)\n",
    "output, hidden = decoder(input_, hidden)\n",
    "print(hidden.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(tensor_pair):\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()    \n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_input = tensor_pair[0].unsqueeze(1)\n",
    "    _, encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([target_lang.word2index['<START>']]).unsqueeze(1)   \n",
    "    \n",
    "    loss = 0\n",
    "    for target_word in tensor_pair[1].unsqueeze(1):        \n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        loss += lossfn(decoder_output, target_word)\n",
    "        _, pred_word = decoder_output.topk(1)\n",
    "        \n",
    "        if pred_word[0][0].item() == target_lang.word2index['<END>']:\n",
    "            break\n",
    "            \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_word.unsqueeze(1)\n",
    "        else:\n",
    "            decoder_input = pred_word\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()  \n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "loss_track = []\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for pair in tqdm(pairs_train):\n",
    "        tensor_pair = sentence2tensor_pair(pair)\n",
    "        avg_loss += train(tensor_pair)\n",
    "    avg_loss /= len(pairs_train)\n",
    "    print('Epoch: ', epoch, '| Loss: ', avg_loss)\n",
    "    loss_track.append(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ee96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ad303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tensor_pair):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_input = tensor_pair[0].unsqueeze(1)\n",
    "    _, encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([target_lang.word2index['<START>']]).unsqueeze(1)\n",
    "    \n",
    "    loss = 0\n",
    "    max_len = 50\n",
    "    translation = []\n",
    "    for k in range(max_len):    \n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        _, pred_word = decoder_output.topk(1)\n",
    "        \n",
    "        translation.append(pred_word[0][0].item())\n",
    "        if pred_word[0][0].item() == target_lang.word2index['<END>']:\n",
    "            break\n",
    "        decoder_input = pred_word\n",
    "    \n",
    "    return translation\n",
    "\n",
    "def get_translation(text):\n",
    "    tensor_pair = sentence2tensor_pair((text,''))\n",
    "    translation = test(tensor_pair)\n",
    "    return ' '.join([target_lang.index2word[idx] for idx in translation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03726ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs_test[0:5]:\n",
    "    print('Source: ', pair[0])\n",
    "    print('Target: ', pair[1])\n",
    "    tensor_pair = sentence2tensor_pair(pair)\n",
    "    translation = test(tensor_pair)\n",
    "    res = ' '.join([target_lang.index2word[idx] for idx in translation])\n",
    "    print('Result: ', res)\n",
    "    print('\\n----------------------------------------------------------------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
