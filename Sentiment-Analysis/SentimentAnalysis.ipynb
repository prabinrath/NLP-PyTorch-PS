{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43f11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "from torchtext import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd20e8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Sentiment                                      SentimentText\n",
       " 0           pos  @amyrenea omg so am I lol I fell asleep when i...\n",
       " 1           neg               @Adrienne_Bailon I want a shout out \n",
       " 2           neg  @Anonymousboy03 Plans for school stuff &amp; a...\n",
       " 3           neg  ... has hit a writer's block .. am loosing my ...\n",
       " 4           neg  ... trying to find people I know! I`m bored, i...\n",
       " ...         ...                                                ...\n",
       " 39995       pos   #robotpickuplines are so funny. check them out. \n",
       " 39996       pos  @annyo84 awh thankss.  yeah, i understand what...\n",
       " 39997       pos  @AmbiguityX ohh you're in twin cities?  i luv ...\n",
       " 39998       neg   Dinara lost again in Roland Garros. Why the S...\n",
       " 39999       pos  *yawn* fucking time zones shit. I'm really sic...\n",
       " \n",
       " [40000 rows x 2 columns],\n",
       "      Sentiment                                      SentimentText\n",
       " 0          pos  @aimeesays aww i hope it does fly by because J...\n",
       " 1          neg  #dontyouhate when you JUST painted yur nails a...\n",
       " 2          neg  - @EvertB which one? http://bit.ly/10o8LW, htt...\n",
       " 3          pos  *shriek* Bee almost flew here from window. I'm...\n",
       " 4          pos  @Alyssa_Milano granted if we lose it is to a w...\n",
       " ...        ...                                                ...\n",
       " 9995       neg  @aisforamylynn you're a badass for having a ba...\n",
       " 9996       pos  @acts_rox  I'm not particular about it being f...\n",
       " 9997       pos                     @@j311stp and the same to you!\n",
       " 9998       pos  .@nanere Sheila I heart you!! That &quot;Holly...\n",
       " 9999       neg   not the same without a goodnight....hm. Wish ...\n",
       " \n",
       " [10000 rows x 2 columns])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/sentiment-analysis/tweets/tweets.csv\", encoding=\"latin-1\")\n",
    "data = data.drop(columns=[\"ItemID\", \"SentimentSource\"], axis=1);\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42);\n",
    "train.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed72d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"../datasets/sentiment-analysis/tweets/tweets_train.csv\", index=False);\n",
    "test.to_csv(\"../datasets/sentiment-analysis/tweets/tweets_test.csv\", index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663a4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner']);\n",
    "\n",
    "def clean_tweets(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text);\n",
    "    text = re.sub(r'https?:/\\/\\/S+', ' ', text);\n",
    "    return text.strip();\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [w.text.lower() for w in nlp(clean_tweets(text))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d206e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\play\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "TEXT = Field(tokenize = tokenizer);\n",
    "LABEL = LabelField(dtype = torch.float);\n",
    "\n",
    "datafields = [(\"Sentiment\",LABEL), (\"SentimentText\",TEXT)];\n",
    "trn, tst = TabularDataset.splits(path=\"../datasets/sentiment-analysis/tweets/\",\n",
    "                                train = \"tweets_train.csv\",\n",
    "                                test = \"tweets_test.csv\",\n",
    "                                format = \"csv\",\n",
    "                                skip_header = True,\n",
    "                                fields = datafields);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590ac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, max_size=25000,\n",
    "                vectors='glove.6B.100d',\n",
    "                unk_init=torch.Tensor.normal_);\n",
    "vocab_size = len(TEXT.vocab);\n",
    "LABEL.build_vocab(trn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d70652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 25644), ('the', 12219), ('to', 12111), ('you', 10723), ('a', 9197), ('it', 8440), ('and', 6889), ('my', 6208), ('quot', 5582), ('s', 5565), ('that', 5306), ('is', 5203), ('for', 4971), ('in', 4852), ('t', 4844), ('m', 4683), ('me', 4588), ('of', 4331), ('on', 3918), ('have', 3752), ('so', 3612), ('but', 3506), ('be', 2932), ('not', 2887), ('was', 2775), ('just', 2724), ('can', 2523), ('do', 2418), ('are', 2351), ('your', 2320), ('with', 2269), ('good', 2203), ('like', 2173), ('at', 2131), ('no', 2119), ('this', 2094), ('all', 2069), ('up', 2066), ('now', 2063), ('get', 2044), ('we', 1988), ('u', 1890), ('love', 1885), ('lol', 1864), ('too', 1826), ('what', 1760), ('out', 1742), ('know', 1664), ('nt', 1608), ('amp', 1539)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9829fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'i', 'the', 'to', 'you', 'a', 'it', 'and', 'my']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[0:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0fc489",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64;\n",
    "train_itr, test_itr = BucketIterator.splits((trn, tst),\n",
    "                                            batch_size = batch_size,\n",
    "                                            sort_key = lambda x: len(x.SentimentText),\n",
    "                                            #sort=False,\n",
    "                                            #shuffle=True,\n",
    "                                            sort_within_batch=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea63ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:9\u001b[1;36m\u001b[0m\n\u001b[1;33m    def forward(self, sms_text):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                     output_dim, n_layers, bidirectional, drpout):\n",
    "        super().__init__();\n",
    "        \n",
    "        \n",
    "    def forward(self, sms_text):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
